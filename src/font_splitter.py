#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å­—ä½“æ‹†åˆ†å·¥å…·
æ ¹æ®å­—ç¬¦é¢‘ç‡é¡ºåºå°†å­—ä½“åŒ…æ‹†åˆ†æˆå¤šä¸ªå­å­—ä½“åŒ…
"""

import os
import sys
import math
import re
import time
import json
import hashlib
from fontTools import subset
from fontTools.ttLib import TTFont
import argparse
from typing import List, Tuple, Iterable, Dict, Optional
from datetime import datetime

# CDNé…ç½® - æ ¹æ®å…¬å¸å®é™…æƒ…å†µä¿®æ”¹
CDN_CONFIG = {
    'base_url': 'https://your-company-cdn.com/fonts',  # æ›¿æ¢ä¸ºå®é™…çš„CDNåœ°å€
}

# è¯­è¨€åˆ°unicodeæ–‡ä»¶çš„æ˜ å°„
LANGUAGE_UNICODE_MAP = {
    'zh': 'unicode-zh-CN.txt',  # ç®€ä½“ä¸­æ–‡
    'ja': 'unicode-ja.txt',     # æ—¥æ–‡
    'tc': 'unicode-zh-TW.txt',  # ç¹ä½“ä¸­æ–‡
}

def parse_unicode_ranges_from_text(text: str) -> List[int]:
    """
   
    """
    codepoints: List[int] = []
    seen = set()
    for m in re.finditer(r"unicode-range\s*:\s*([^;]+);", text, flags=re.IGNORECASE):
        ranges_part = m.group(1)
        # å»é™¤ C é£æ ¼å—æ³¨é‡Šï¼Œé¿å…æ³¨é‡ŠæŠŠé€—å·åˆ†å‰²æ‰“ä¹±
        ranges_part = re.sub(r"/\*.*?\*/", "", ranges_part, flags=re.DOTALL)
        # é€—å·åˆ†å‰²æ¯ä¸ªé¡¹
        for item in ranges_part.split(','):
            token = item.strip()
            if not token:
                continue
            # è§£æ U+XXXX æˆ– U+XXXX-YYYY
            m2 = re.match(r"U\+([0-9A-Fa-f]{1,6})(?:-([0-9A-Fa-f]{1,6}))?", token)
            if not m2:
                continue
            start_hex = m2.group(1)
            end_hex = m2.group(2)
            start_cp = int(start_hex, 16)
            end_cp = int(end_hex, 16) if end_hex else start_cp
            if end_cp < start_cp:
                start_cp, end_cp = end_cp, start_cp
            for cp in range(start_cp, end_cp + 1):
                if cp not in seen:
                    seen.add(cp)
                    codepoints.append(cp)
    return codepoints

def parse_unicode_order_file(file_path: str) -> List[str]:
    """
    ä»unicodeé…ç½®æ–‡ä»¶ä¸­è§£æå‡ºæŒ‰å‡ºç°é¡ºåºæ’åˆ—çš„ Unicode
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        cps = parse_unicode_ranges_from_text(content)
        chars: List[str] = []
        for cp in cps:
            try:
                chars.append(chr(cp))
            except Exception:
                # è·³è¿‡æ— æ•ˆç ç‚¹
                pass
        return chars
    except Exception as e:
        print(f"è¯»å–æˆ–è§£æ unicode é¡ºåºæ–‡ä»¶å¤±è´¥: {e}")
        return []

def load_character_list_file(file_path: str) -> List[str]:
    """
    ä»çº¯å­—ç¬¦åˆ—è¡¨æ–‡ä»¶ä¸­è¯»å–å­—ç¬¦ã€‚
    æ–‡ä»¶å†…å®¹åº”è¯¥æ˜¯è¿ç»­çš„å­—ç¬¦å­—ç¬¦ä¸²ï¼Œæ²¡æœ‰å…¶ä»–æ ¼å¼ã€‚
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        return list(content)
    except Exception as e:
        print(f"è¯»å–å­—ç¬¦åˆ—è¡¨æ–‡ä»¶å¤±è´¥: {e}")
        return []

def expand_unicode_ranges_to_chars(ranges: Iterable[str]) -> List[str]:
    """
    å°†å½¢å¦‚ 'U+4e00' æˆ– 'U+4e00-4e0f' çš„ç‰‡æ®µåˆ—è¡¨å±•å¼€ä¸ºå­—ç¬¦åˆ—è¡¨ï¼ˆä¿åºå»é‡ï¼‰ã€‚
    ä»…ç”¨äºé¢„è®¾å—å±•å¼€ã€‚
    """
    seen = set()
    chars: List[str] = []
    for token in ranges:
        m2 = re.match(r"U\+([0-9A-Fa-f]{1,6})(?:-([0-9A-Fa-f]{1,6}))?", token.strip())
        if not m2:
            continue
        start_cp = int(m2.group(1), 16)
        end_cp = int(m2.group(2), 16) if m2.group(2) else start_cp
        if end_cp < start_cp:
            start_cp, end_cp = end_cp, start_cp
        for cp in range(start_cp, end_cp + 1):
            if cp not in seen:
                seen.add(cp)
                try:
                    chars.append(chr(cp))
                except Exception:
                    pass
    return chars

def merge_orders_keep_first(*orders: Iterable[str]) -> List[str]:
    """
    åˆå¹¶å¤šä¸ªå­—ç¬¦åºåˆ—ï¼Œä¿ç•™é¦–æ¬¡å‡ºç°çš„é¡ºåºï¼Œåç»­é‡å¤å°†è¢«å¿½ç•¥ã€‚
    """
    seen = set()
    merged: List[str] = []
    for order in orders:
        if not order:
            continue
        for ch in order:
            if ch not in seen:
                seen.add(ch)
                merged.append(ch)
    return merged

def build_order_from_corpus(paths: List[str]) -> List[str]:
    """
    ä»æ–‡æœ¬è¯­æ–™æ„å»ºå­—ç¬¦é¢‘ç‡é¡ºåºï¼šæŒ‰å‡ºç°æ¬¡æ•°ä»é«˜åˆ°ä½ï¼Œå‡ºç°æ¬¡æ•°ç›¸åŒæ—¶æŒ‰å…ˆå‡ºç°é¡ºåºç¨³å®šæ’åºã€‚
    ä»…ç»Ÿè®¡ BMP å¸¸ç”¨å¯æ‰“å°å­—ç¬¦ï¼ˆä¸é™å®šè¯­è¨€ï¼‰ã€‚
    """
    counts: Dict[str, int] = {}
    first_index: Dict[str, int] = {}
    idx = 0
    for p in paths:
        try:
            with open(p, 'r', encoding='utf-8') as f:
                for line in f:
                    for ch in line:
                        # è¿‡æ»¤æ§åˆ¶å­—ç¬¦
                        if ch <= '\u001f':
                            continue
                        counts[ch] = counts.get(ch, 0) + 1
                        if ch not in first_index:
                            first_index[ch] = idx
                        idx += 1
        except Exception as e:
            print(f"è­¦å‘Š: è¯»å–è¯­æ–™å¤±è´¥ {p}: {e}")
    # æŒ‰é¢‘æ¬¡é™åºã€é¦–æ¬¡å‡ºç°å‡åºæ’åº
    ordered = sorted(counts.keys(), key=lambda c: (-counts[c], first_index[c]))
    return ordered

PRESET_BLOCKS: Dict[str, List[str]] = {
    # åŸºç¡€æ—¥æ–‡å—ï¼šå¹³å‡åã€ç‰‡å‡åã€ç‰‡å‡åéŸ³æ ‡æ‰©å±•ã€åŠè§’ç‰‡å‡å
    'jp-basic': [
        'U+3040-309F',  # Hiragana
        'U+30A0-30FF',  # Katakana
        'U+31F0-31FF',  # Katakana Phonetic Extensions
        'U+FF66-FF9F',  # Halfwidth Katakana
        # å¸¸ç”¨ CJK ç»Ÿä¸€è¡¨æ„æ–‡å­—ï¼ˆæ±‰å­—ï¼Œå…±ç”¨ï¼‰ï¼ŒæŒ‰åŒºå—æ•´ä½“è¿½åŠ ï¼Œç”±ä½ çš„å¤–éƒ¨é¡ºåºå†åšäº¤é›†
        'U+4E00-9FFF',  # CJK Unified Ideographs
    ],
    # ç¹ä½“åŸºç¡€å—ï¼šCJK ç»Ÿä¸€è¡¨æ„æ–‡å­—åŠå…¼å®¹/æ‰©å±•å¸¸ç”¨åŒºï¼ˆç²—ç²’åº¦ï¼‰
    'tc-basic': [
        'U+4E00-9FFF',  # CJK Unified Ideographs
        'U+3400-4DBF',  # CJK Extension A
        'U+F900-FAFF',  # CJK Compatibility Ideographs
        'U+2F800-2FA1F',  # CJK Compatibility Ideographs Supplement
    ],
}

def analyze_font_characters(font_path: str) -> List[str]:
    """
    åˆ†æå­—ä½“æ–‡ä»¶ä¸­åŒ…å«çš„å­—ç¬¦
    è¿”å›å­—ä½“ä¸­å®é™…å­˜åœ¨çš„å­—ç¬¦åˆ—è¡¨
    """
    try:
        font = TTFont(font_path)
        available_chars = []
        
        # è·å–å­—ä½“ä¸­çš„æ‰€æœ‰å­—ç¬¦
        if 'cmap' in font:
            for table in font['cmap'].tables:
                if hasattr(table, 'cmap'):
                    for unicode_val, glyph_name in table.cmap.items():
                        if unicode_val > 0:  # æ’é™¤æ§åˆ¶å­—ç¬¦
                            char = chr(unicode_val)
                            available_chars.append(char)
        
        return list(set(available_chars))  # å»é‡
    except Exception as e:
        print(f"åˆ†æå­—ä½“å­—ç¬¦æ—¶å‡ºé”™: {e}")
        return []

def filter_available_chars(font_chars: List[str], ordered_chars: List[str]) -> List[str]:
    """
    æ ¹æ®å¤–éƒ¨é¡ºåºä¸å­—ä½“å®é™…å¯ç”¨å­—ç¬¦åšæœ‰åºäº¤é›†ï¼Œä»…ä¿ç•™äºŒè€…éƒ½åŒ…å«çš„å­—ç¬¦ï¼Œä¿åºä¸è¡¥é½ã€‚
    """
    font_char_set = set(font_chars)
    return [c for c in ordered_chars if c in font_char_set]

def split_characters_into_chunks(chars: List[str], num_chunks: int) -> List[List[str]]:
    """
    å°†å­—ç¬¦åˆ—è¡¨åˆ†å‰²æˆæŒ‡å®šæ•°é‡çš„å—
    è¿”å›å­—ç¬¦å—åˆ—è¡¨
    """
    if num_chunks <= 0:
        raise ValueError("å—æ•°é‡å¿…é¡»å¤§äº0")
    
    if num_chunks >= len(chars):
        # å¦‚æœå—æ•°é‡å¤§äºç­‰äºå­—ç¬¦æ•°é‡ï¼Œæ¯ä¸ªå­—ç¬¦ä¸€ä¸ªå—ï¼ˆå¯èƒ½ä¼šå¤šå‡ºç©ºå—ï¼Œåç»­è¿‡æ»¤ï¼‰
        return [[char] for char in chars]

    # å°†å­—ç¬¦ç²¾ç¡®åœ°åˆ’åˆ†ä¸º num_chunks ä¸ªè¿ç»­å—ï¼Œå°½é‡å¹³å‡åˆ†é…
    total = len(chars)
    base = total // num_chunks
    rem = total % num_chunks
    chunks: List[List[str]] = []
    idx = 0
    for i in range(num_chunks):
        size = base + (1 if i < rem else 0)
        if size == 0:
            chunks.append([])
            continue
        chunk = chars[idx:idx + size]
        chunks.append(chunk)
        idx += size
    # å»æ‰å°¾éƒ¨å¯èƒ½çš„ç©ºå—
    while chunks and not chunks[-1]:
        chunks.pop()
    return chunks

def create_font_subset(input_font_path: str, output_path: str, chars: List[str]) -> bool:
    """
    åˆ›å»ºå­—ä½“å­é›†
    è¿”å›æ˜¯å¦æˆåŠŸ
    """
    try:
        # åŠ è½½å­—ä½“
        font = TTFont(input_font_path)
        
        # åˆ›å»ºå­é›†åŒ–å™¨
        subsetter = subset.Subsetter()
        
        # è®¾ç½®è¦ä¿ç•™çš„å­—ç¬¦
        char_text = ''.join(chars)
        subsetter.populate(text=char_text)
        
        # æ‰§è¡Œå­é›†åŒ–
        subsetter.subset(font)
        
        # ä¿å­˜å­é›†å­—ä½“
        font.save(output_path)
        
        return True
    except Exception as e:
        print(f"åˆ›å»ºå­—ä½“å­é›†æ—¶å‡ºé”™: {e}")
        return False

def codepoints_to_unicode_ranges(codepoints: List[int]) -> List[str]:
    """
    å°†å·²æ’åºçš„ç ç‚¹åˆ—è¡¨å‹ç¼©ä¸º CSS unicode-range ç‰‡æ®µåˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š
    [0x4E00,0x4E01,0x4E02,0x4E05] -> ['U+4E00-4E02','U+4E05']
    """
    if not codepoints:
        return []
    # ä¸æ’åºï¼Œä¸¥æ ¼æŒ‰ç…§ä¼ å…¥é¡ºåºå‹ç¼©ç›¸é‚»ç ç‚¹ï¼Œä»¥ä¿æŒä¸å¤–éƒ¨é¡ºåºä¸€è‡´çš„å±•ç¤º
    cps = list(codepoints)
    ranges: List[str] = []
    start = cps[0]
    prev = cps[0]
    for cp in cps[1:]:
        if cp == prev + 1:
            prev = cp
            continue
        # ç»“æŸä¸Šä¸€ä¸ªæ®µ
        if start == prev:
            ranges.append(f"U+{start:x}")
        else:
            ranges.append(f"U+{start:x}-{prev:x}")
        start = prev = cp
    # æœ€åä¸€æ®µ
    if start == prev:
        ranges.append(f"U+{start:x}")
    else:
        ranges.append(f"U+{start:x}-{prev:x}")
    return ranges

# Mock CDNä¸Šä¼ æ¥å£ - åç»­æ›¿æ¢ä¸ºå®é™…OSSæ¥å£
def mock_upload_to_cdn(font_file_path: str, cdn_base_url: str, language: str) -> str:
    """
    Mock CDNä¸Šä¼ æ¥å£
    å®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºå…¬å¸çš„OSSä¸Šä¼ æ¥å£
    
    å‚æ•°:
    - font_file_path: æœ¬åœ°å­—ä½“æ–‡ä»¶è·¯å¾„
    - cdn_base_url: CDNåŸºç¡€URL
    - language: è¯­è¨€æ ‡è¯†
    
    è¿”å›:
    - CDN URL
    """
    # ç”Ÿæˆæ–‡ä»¶åhashï¼Œé¿å…é‡å
    with open(font_file_path, 'rb') as f:
        file_hash = hashlib.md5(f.read()).hexdigest()[:8]
    
    filename = os.path.basename(font_file_path)
    name, ext = os.path.splitext(filename)
    cdn_filename = f"{name}_{file_hash}{ext}"
    
    # Mock CDN URL
    cdn_url = f"{cdn_base_url}/{language}/{cdn_filename}"
    
    print(f"  ğŸ“¤ Mockä¸Šä¼ : {font_file_path} -> {cdn_url}")
    
    # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„OSSä¸Šä¼ æ¥å£
    # ä¾‹å¦‚: upload_result = oss_client.upload_file(font_file_path, cdn_filename)
    
    return cdn_url

def generate_css_file(subset_info_list: List[Dict], cdn_base_url: str, font_family: str, output_css_path: str) -> bool:
    """
    ç”ŸæˆåŒ…å«CDNåœ°å€çš„CSSæ–‡ä»¶
    
    å‚æ•°:
    - subset_info_list: å­é›†ä¿¡æ¯åˆ—è¡¨
    - cdn_base_url: CDNåŸºç¡€URL
    - font_family: å­—ä½“æ—åç§°
    - output_css_path: è¾“å‡ºCSSæ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    - bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        css_rules = []
        
        # æ·»åŠ æ–‡ä»¶å¤´æ³¨é‡Š
        header = f"""/* 
 * å­—ä½“CSSæ–‡ä»¶ - {font_family}
 * ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
 * CDNåŸºç¡€åœ°å€: {cdn_base_url}
 * åŒ…å« {len(subset_info_list)} ä¸ªå­—ä½“å­é›†
 */

"""
        
        for subset_info in subset_info_list:
            subset_num = subset_info['subset_num']
            unicode_ranges = subset_info['unicode_ranges']
            cdn_url = subset_info['cdn_url']
            language = subset_info.get('language', 'unknown')
            
            # ç”Ÿæˆ@font-faceè§„åˆ™
            font_face = f"""@font-face {{
  font-family: {font_family};
  font-weight: 400;
  font-display: swap;
  src: url("{cdn_url}");
  unicode-range: {unicode_ranges};
}}"""
            
            css_rules.append(font_face)
        
        # å†™å…¥CSSæ–‡ä»¶
        with open(output_css_path, 'w', encoding='utf-8') as f:
            f.write(header)
            f.write('\n\n'.join(css_rules))
        
        print(f"âœ… CSSæ–‡ä»¶å·²ç”Ÿæˆ: {output_css_path}")
        return True
        
    except Exception as e:
        print(f"ç”ŸæˆCSSæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

def split_font(input_font_path: str, output_folder: str, num_chunks: int = 200, preferred_order: List[str] = None, chars_per_chunk: int | None = None, font_family: str = "æœ‰çˆ±é­”å…½åœ†ä½“-M", language: str = "mixed") -> bool:
    """
    æ‹†åˆ†å­—ä½“æ–‡ä»¶
    
    å‚æ•°:
    - input_font_path: è¾“å…¥å­—ä½“æ–‡ä»¶è·¯å¾„
    - output_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    - num_chunks: è¦æ‹†åˆ†çš„å—æ•°é‡ï¼Œé»˜è®¤200
    
    è¿”å›:
    - bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not os.path.exists(input_font_path):
            print(f"é”™è¯¯: è¾“å…¥å­—ä½“æ–‡ä»¶ä¸å­˜åœ¨: {input_font_path}")
            return False
        
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
        os.makedirs(output_folder, exist_ok=True)
        
        print(f"å¼€å§‹åˆ†æå­—ä½“æ–‡ä»¶: {input_font_path}")
        
        # åˆ†æå­—ä½“ä¸­çš„å­—ç¬¦
        font_chars = analyze_font_characters(input_font_path)
        print(f"å­—ä½“ä¸­åŒ…å« {len(font_chars)} ä¸ªå­—ç¬¦")
        
        # å¿…é¡»æä¾›å¤–éƒ¨é¡ºåº
        if not preferred_order:
            print("é”™è¯¯: æœªæä¾›å¤–éƒ¨å­—ç¬¦é¡ºåºï¼ˆ--unicode-order-fileï¼‰")
            return False
        frequency_order = preferred_order
        
        # è¿‡æ»¤å¯ç”¨å­—ç¬¦å¹¶æŒ‰å¤–éƒ¨é¡ºåºæ’åºï¼ˆä¸¥æ ¼äº¤é›†ï¼Œä¸è¡¥é½ï¼‰
        available_chars = filter_available_chars(font_chars, frequency_order)
        print(f"æŒ‰é¢‘ç‡æ’åºçš„å¯ç”¨å­—ç¬¦: {len(available_chars)} ä¸ª")
        
        # è®¡ç®—å®é™…å—æ•°ï¼šä¼˜å…ˆæŒ‰æ¯å—å­—ç¬¦æ•°ä¼°ç®—ï¼›å¦åˆ™ä½¿ç”¨æŒ‡å®šå—æ•°
        effective_num_chunks = num_chunks
        if chars_per_chunk and chars_per_chunk > 0:
            effective_num_chunks = max(1, math.ceil(len(available_chars) / chars_per_chunk))

        # åˆ†å‰²å­—ç¬¦
        char_chunks = split_characters_into_chunks(available_chars, effective_num_chunks)
        print(f"å°†å­—ç¬¦åˆ†å‰²æˆ {len(char_chunks)} ä¸ªå—")
        
        # è·å–åŸºç¡€æ–‡ä»¶å
        base_name = os.path.splitext(os.path.basename(input_font_path))[0]
        file_ext = os.path.splitext(input_font_path)[1]
        
        # åˆ›å»ºå­é›†å­—ä½“å¹¶æ”¶é›†ä¿¡æ¯
        success_count = 0
        subset_info_list = []
        
        for i, chunk in enumerate(char_chunks, 1):
            output_filename = f"{base_name}_subset_{i:03d}{file_ext}"
            output_path = os.path.join(output_folder, output_filename)
            
            print(f"åˆ›å»ºå­é›† {i}/{len(char_chunks)}: {output_filename} (åŒ…å« {len(chunk)} ä¸ªå­—ç¬¦)")
            
            if create_font_subset(input_font_path, output_path, chunk):
                success_count += 1
                
                # è®¡ç®—unicode-range
                cps = [ord(c) for c in chunk]
                ranges = codepoints_to_unicode_ranges(cps)
                unicode_ranges = ",".join(ranges)
                
                # ä¸Šä¼ åˆ°CDNå¹¶è·å–CDN URL
                cdn_url = mock_upload_to_cdn(output_path, CDN_CONFIG['base_url'], language)
                
                # æ”¶é›†å­é›†ä¿¡æ¯
                subset_info = {
                    'subset_num': i,
                    'char_count': len(chunk),
                    'characters': ''.join(chunk),
                    'unicode_ranges': unicode_ranges,
                    'local_path': output_path,
                    'cdn_url': cdn_url,
                    'language': language
                }
                subset_info_list.append(subset_info)
            else:
                print(f"è­¦å‘Š: åˆ›å»ºå­é›† {i} å¤±è´¥")
        
        print(f"\næ‹†åˆ†å®Œæˆ! æˆåŠŸåˆ›å»º {success_count}/{len(char_chunks)} ä¸ªå­é›†å­—ä½“")
        
        # ç”Ÿæˆå­—ç¬¦æ˜ å°„æ–‡ä»¶
        mapping_file = os.path.join(output_folder, f"{base_name}_character_mapping.txt")
        with open(mapping_file, 'w', encoding='utf-8') as f:
            f.write(f"å­—ä½“æ‹†åˆ†å­—ç¬¦æ˜ å°„ - {base_name}\n")
            f.write("=" * 50 + "\n\n")
            
            for subset_info in subset_info_list:
                f.write(f"å­é›† {subset_info['subset_num']:03d} ({subset_info['char_count']} ä¸ªå­—ç¬¦):\n")
                f.write(f"å­—ç¬¦: {subset_info['characters']}\n")
                f.write("unicode-range: " + subset_info['unicode_ranges'] + ";\n")
                if subset_info['cdn_url']:
                    f.write(f"CDNåœ°å€: {subset_info['cdn_url']}\n")
                f.write("-" * 30 + "\n")
        
        print(f"å­—ç¬¦æ˜ å°„æ–‡ä»¶å·²ä¿å­˜: {mapping_file}")
        
        # ç”ŸæˆCSSæ–‡ä»¶
        if subset_info_list:
            css_filename = f"{base_name}_{language}.css"
            css_path = os.path.join(output_folder, css_filename)
            
            if generate_css_file(subset_info_list, CDN_CONFIG['base_url'], font_family, css_path):
                print(f"ğŸ‰ CSSæ–‡ä»¶å·²ç”Ÿæˆ: {css_path}")
                print(f"ğŸ’¡ ä½¿ç”¨æ–¹æ³•: åœ¨HTMLä¸­å¼•å…¥æ­¤CSSæ–‡ä»¶ï¼Œç„¶åè®¾ç½® font-family: '{font_family}'")
            else:
                print("âŒ CSSæ–‡ä»¶ç”Ÿæˆå¤±è´¥")
        
        return success_count > 0
        
    except Exception as e:
        print(f"æ‹†åˆ†å­—ä½“æ—¶å‡ºé”™: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='å­—ä½“æ‹†åˆ†å·¥å…· - æŒ‰è¯­è¨€è‡ªåŠ¨é€‰æ‹©unicodeé¡ºåºæ‹†åˆ†æˆå¤šä¸ªå­é›†å¹¶ç”ŸæˆCDN CSS')
    parser.add_argument('input_font', help='è¾“å…¥å­—ä½“æ–‡ä»¶è·¯å¾„ (.ttf, .otf)')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„', default='./splitRes')
    parser.add_argument('-n', '--num-chunks', type=int, help='æ‹†åˆ†çš„å—æ•°é‡ï¼ˆä¸ --chars-per-chunk äº’æ–¥ï¼Œè‹¥åŒæ—¶æä¾›åˆ™ä»¥ --chars-per-chunk ä¸ºå‡†ï¼‰', default=200)
    parser.add_argument('--chars-per-chunk', type=int, help='æ¯ä¸ªå­é›†çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆä¼˜å…ˆäº --num-chunks è®¡ç®—å®é™…å—æ•°ï¼‰')
    parser.add_argument('--character-list-file', action='append', required=False, help='ä»çº¯å­—ç¬¦åˆ—è¡¨æ–‡ä»¶è¯»å–å­—ç¬¦é¡ºåºï¼ˆå¯å¤šæ¬¡æä¾›ï¼‰')
    parser.add_argument('--include-blocks', nargs='*', default=[], help='é¢„è®¾å—ï¼šjp-basic, tc-basic ç­‰ï¼Œå¯å¤šé€‰')
    parser.add_argument('--auto-order-from-corpus', nargs='*', help='ä»æ–‡æœ¬è¯­æ–™è‡ªåŠ¨æ„å»ºå­—ç¬¦é¢‘ç‡é¡ºåºï¼ˆè¿½åŠ åˆ°æœ«å°¾ï¼‰')
    parser.add_argument('--language', choices=['zh','ja','tc'], required=True, help='è¯­è¨€ç±»å‹ï¼šzh(ç®€ä¸­), ja(æ—¥æ–‡), tc(ç¹ä½“)')
    
    # å­—ä½“ç›¸å…³å‚æ•°
    parser.add_argument('--font-family', help='å­—ä½“æ—åç§°', default='æœ‰çˆ±é­”å…½åœ†ä½“-M')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input_font):
        print(f"é”™è¯¯: è¾“å…¥å­—ä½“æ–‡ä»¶ä¸å­˜åœ¨: {args.input_font}")
        sys.exit(1)
    
    # æ ¹æ®è¯­è¨€è‡ªåŠ¨é€‰æ‹©unicodeæ–‡ä»¶
    lang = args.language
    preferred_order_chars: List[str] = []
    
    # è‡ªåŠ¨è¯»å–å¯¹åº”è¯­è¨€çš„unicodeæ–‡ä»¶
    unicode_file = LANGUAGE_UNICODE_MAP[lang]
    if os.path.exists(unicode_file):
        print(f"è‡ªåŠ¨è¯»å– {lang} è¯­è¨€çš„å­—ç¬¦é¡ºåºæ–‡ä»¶: {unicode_file}")
        chars = parse_unicode_order_file(unicode_file)
        preferred_order_chars = merge_orders_keep_first(preferred_order_chars, chars)
    else:
        print(f"è­¦å‘Š: æ‰¾ä¸åˆ° {lang} è¯­è¨€çš„unicodeæ–‡ä»¶: {unicode_file}")
        # ä½¿ç”¨é¢„è®¾å—ä½œä¸ºå¤‡ç”¨
        if lang == 'ja':
            chars = expand_unicode_ranges_to_chars(PRESET_BLOCKS['jp-basic'])
            preferred_order_chars = merge_orders_keep_first(preferred_order_chars, chars)
        elif lang == 'tc':
            chars = expand_unicode_ranges_to_chars(PRESET_BLOCKS['tc-basic'])
            preferred_order_chars = merge_orders_keep_first(preferred_order_chars, chars)
    
    # å¯é€‰ï¼šæ·»åŠ é¢å¤–çš„å­—ç¬¦åˆ—è¡¨æ–‡ä»¶
    if args.character_list_file:
        for p in args.character_list_file:
            print(f"è¯»å–é¢å¤–å­—ç¬¦åˆ—è¡¨æ–‡ä»¶: {p}")
            chars = load_character_list_file(p)
            preferred_order_chars = merge_orders_keep_first(preferred_order_chars, chars)
    
    # å¯é€‰ï¼šæ·»åŠ é¢„è®¾å—
    if args.include_blocks:
        for key in args.include_blocks:
            if key not in PRESET_BLOCKS:
                print(f"è­¦å‘Š: æœªçŸ¥é¢„è®¾å— {key}")
                continue
            block_ranges = PRESET_BLOCKS[key]
            chars = expand_unicode_ranges_to_chars(block_ranges)
            preferred_order_chars = merge_orders_keep_first(preferred_order_chars, chars)
    
    # å¯é€‰ï¼šä»è¯­æ–™æ„å»ºé¢‘ç‡é¡ºåº
    if args.auto_order_from_corpus:
        auto_chars = build_order_from_corpus(args.auto_order_from_corpus)
        preferred_order_chars = merge_orders_keep_first(auto_chars, preferred_order_chars)
    
    print(f"èšåˆåçš„é¡ºåºå­—ç¬¦æ•°: {len(preferred_order_chars)}")

    # æ‰§è¡Œæ‹†åˆ†ï¼ˆè‹¥ä»ä¸ºç©ºï¼Œæç¤ºé”™è¯¯ï¼‰
    if not preferred_order_chars:
        print(f"é”™è¯¯: æ— æ³•ä¸º {lang} è¯­è¨€æ„å»ºå­—ç¬¦é¡ºåºã€‚è¯·æ£€æŸ¥å¯¹åº”çš„unicodeæ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")
        sys.exit(1)

    # æ‰§è¡Œæ‹†åˆ†
    # æ ¹æ®è¯­è¨€åŠ ä¸€å±‚ç›®å½•
    effective_output = os.path.join(args.output, args.language)
    os.makedirs(effective_output, exist_ok=True)

    print(f"å¼€å§‹æ‹†åˆ†å­—ä½“: {args.input_font}")
    print(f"è¾“å‡ºæ–‡ä»¶å¤¹: {effective_output}")
    if args.chars_per_chunk and args.chars_per_chunk > 0:
        print(f"æŒ‰æ¯å—æœ€å¤š {args.chars_per_chunk} ä¸ªå­—ç¬¦è®¡ç®—å—æ•°")
    else:
        print(f"æ‹†åˆ†æ•°é‡: {args.num_chunks}")
    
    # CDNé…ç½®ä¿¡æ¯
    print(f"CDNåŸºç¡€åœ°å€: {CDN_CONFIG['base_url']}")
    print("ğŸ“¤ å°†è‡ªåŠ¨ä¸Šä¼ å­—ä½“æ–‡ä»¶åˆ°CDNå¹¶ç”ŸæˆCSS")
    
    print("-" * 50)
    start_time = time.time()
    success = split_font(
        args.input_font,
        effective_output,
        args.num_chunks,
        preferred_order=preferred_order_chars,
        chars_per_chunk=args.chars_per_chunk,
        font_family=args.font_family,
        language=args.language
    )
    total_seconds = time.time() - start_time
    
    if success:
        print("\nâœ… å­—ä½“æ‹†åˆ†å®Œæˆ!")
        print(f"æ€»è€—æ—¶: {total_seconds:.2f} ç§’ (~{total_seconds/60:.2f} åˆ†é’Ÿ)")
        
        print("\nğŸ‰ å­—ä½“å·²ä¸Šä¼ åˆ°CDNï¼ŒCSSæ–‡ä»¶å·²ç”Ÿæˆ!")
        print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print(f"   1. åœ¨HTMLä¸­å¼•å…¥ç”Ÿæˆçš„CSSæ–‡ä»¶")
        print(f"   2. è®¾ç½® font-family: '{args.font_family}'")
    else:
        print("\nâŒ å­—ä½“æ‹†åˆ†å¤±è´¥!")
        sys.exit(1)

if __name__ == "__main__":
    main()
